Abstract 
As in the case for validity theory which has evolved in response to the increased use of assessments (Kane, 2006; Messick, 1989) in large-scale testing context, the increase attention if formative assessments has taken the concept of validity into focus. While the concept is well examined and constructed in the context of testing, the attempts to understand the role of them in formative assessment practice are still green. This paper aims to clarify the current perspectives on the concepts of validity in the context of formative assessment and to contribute to the discussion by examining the elements of interpretation as situated in validity theory for formative assessment. As an end product, it introduces a coherent validity framework, process validation, in the context of formative assessment.
Introduction
The purpose is the most critical element shaping the test design and criteria for evaluating the validity of the test (Shepard, 2003, p. 122). In the large scale context, assessment is used to monitor the performances of students and used for the accountability purposes (e.g. for teachers).  To achieve the fairness, validity is seen as critical and prerequisite tool referring to prove meaningful inferences, and effective and intended use of them. In connection to the relatively long history of testing, the concept of validity is well examined and constructed (even if not solved completely) in the context of large scale assessments.
 The purpose of formative  assessment can be conceived from its definition(s). While it is a concept which is still evolving, the current common understanding of formative assessment defines it as a practice where the information is extracted from the process and used by teachers and/or students, and it provides feedback to make changes during/in instruction (e.g. William and Black, 2009). The differential characteristics of formative assessment such as close connection to the instruction, active role of both teachers and students, and context dependency (e.g. Brookhart, 2003; McMillan, 2003) leads researchers to reshape the conceptualization of the validity in that context. However, the attempts to understand the role of validity in the formative assessment practice are still green (e.g. Bennet, 2011; Moss, 2003). 
 The first part of the paper examines the current test validity approach and related processes to collect the evidence for large scale assessments. The second part discusses what constitutes the formative assessment and points out the differences with large scale with respect to concept of validity. The last part attempts toward a coherent validity theory by focusing on its on very nature as a process and by using the system notion provided by Bennet (2011).  
Validity in large scale context  
In the measurement literature, what it means a test or assessment to be valid is viewed from two main perspectives. One approach states that validity is a property of measurement instrument (e.g. Borsboom et. al, 2004). According to other approach (e.g. Messick, 1989; Shepard, 1993; Kane, 2006) validity is about the interpretations and uses which are based on the test scores. For the current interest, the attention is given to the latter one.
Messick (1989) defined validity as the “integrated evaluative judgment of the degree to which empirical evidence and theoretical rationales support the adequacy and appropriateness of inferences and actions based on test scores or other modes of assessment” (p. 13). The definition emphasized validity as an evaluative process (rather than a property of the test) which focuses on inferences derived from assessment scores and the actions resulting from those inferences. This view can be seen as the expanded version of Borsboom’s perspective by including it, as well. Interpretations are varied involving the meaning of the test scores, relevance, utility and social consequences. The evidence about the meaning of test scores is collected through construct definition, content analysis (e.g. via expert panels, alignment with standards), results of model-fit analysis with data, correlation analysis with other constructs that are theoretically related or unrelated with construct under consideration. Also, additional evidence is required about value implications on tests preparation and uses of test scores. For instance, in a replacement test for remedial class, it need to be examined whether the consequences of the replacement is beneficial for students. Two threats to validity making the test scores and their interpretations unclear are defined. First one is construct underrepresentation which refers to the imperfectness of test to cover all aspects of the construct defined to be measured (e.g. developing a general math test with only algebra items). The second is construct irrelevant variance where, for example, if there are construct-irrelevant easy items involving 'test-wise' solutions, it gives an advantage to 'test-wise' examinees that their scores invalidly high (Messick, 1989). The examination of the item biases and differential functioning of the items with respect to population subgroups also required as evidence against this threat.
Kane (2006) takes the argument based approach by specifying two kinds of arguments. On the one hand, it requires an interpretive argument, which specifies the proposed interpretations and uses of assessment results. The interpretive argument sets out the network of inferences and assumptions leading from the observed performances to the conclusions and decisions based on the performances. An argument needs to be clearly defined, must be coherent in a sense that conclusions follow reasonable from assumptions, and assumptions must be plausible which includes investigation of counterarguments. On the other hand, there is the validity argument, which provides an evaluation of the interpretive argument usually by quantitative means (Kane, 2006). In a similar and more practical manner, Shepard (1993) broadens the definition and proposes the questions of “what does the testing practice claim to do?”, “what are arguments for and against the intended aims of the test”, and “what does the test do in the system other than what it claims, for good or bad?” to guide the validity evaluations (p.429). She argues validity to be a process of making investigations to answer these questions. 
Formative assessment 
Black and William (1998) provided the most cited definition where formative assessment is “all those activities undertaken by teachers, and/or by their students, which provide information to be used as feedback to modify the teaching and learning activities in which they are engaged (pp. 7-8). The critical components in their definition are the use of feedback to inform the instruction and the active role of both teachers and students. Their latter definition also includes the role of peers (e.g. Black & William, 2009). The process is defined to be interactive with exchanges between teacher-student and student-student by the means of feedback. The definition also refers to embedded nature of assessment in instruction where it becomes an integral part of the learning environment. Shepard (2000) underlies this where assessment and instruction need to be interwoven in such a way that line between them blurs. Therefore, theories underpinning the formative assessment or its elements are mostly referred to knowledge construction and the group learning (e.g. Brookhart, 2004). For example, active student involvement by self-evaluation (e.g. Shepard, 2000) is related to the applying feedback they get during the instruction.
There are several issues that are not under a common agreement of the formative assessment. One is domain consideration. Some definitions take the view of the domain free notion (e.g. Black & William, 2009) while others emphasize the specific characteristics of different domains (e.g. Bennet, 2011). Another issue is a degree of demand for effectiveness that at one end, formative assessment is seen as an informing tool for instruction and on other end, it puts the condition on improvement in student learning (Furtak et al., in progress).
Currently, Bennett (2011) suggests formative assessment to be a combination of task, instrument, and process. He brings a system conception to the formative assessment consisting of “thoughtful integration of process and purposefully designed methodology and instrumentation” (p.7).  In this paper, I conceive the formative assessment as it is described by Bennet where it is a system embracing the process and instrument with the aim of taking action to improve learning. 
 By the very nature of formative assessment, the meaning and the use of terms of validity might differ for teachers in the context of the classroom assessment (McMillan, 2003). It is apparent that classroom assessment is very closely connected to the instruction than it becomes in large scale testing. This differs from the objective aim of the large scale assessment that it is more subjective in formative assessment (McMillan, 2003). In classroom, it is more important to recognize and adapt the individual differences into the assessments such as students’ interest and background while the standardization is the key concept in the context of large scale assessments such that it is accepted as a source of invalidity if the tests favor a group of students based on their interests. The need in classroom assessment is matching level of difficulty with the current level of student’s understanding, providing feedback, and emphasizing the intrinsic motivation and self-evaluation which also change the criterion about the achievement (e.g. Brookhart, 2003). The context dependency of the formative assessment is differential point that in large scale assessment it is considered to be a source of irrelevant variance with the aim of generalizing across contexts. However, in classroom assessment items and tasks are nested in the instructional environment (Brookhart, 2003, p.5) and the assessment is attended to the interactions. The question of who makes the action also differs in both contexts such that as in classroom both teachers and students make inferences and take action.  
 The lifecycle of the formative assessment is short where the information provided that immediately becomes a part of learning environment. Hence, the types of the statistical analysis for the large scale context will not be applicable for the classroom assessment. 
General tendency in formative assessment literature is that goal of validity in classroom assessment is to understand the role of assessment information (e.g. was feedback helpful) with the reshaped meaning of content representativeness such that it matches with instructional objectives and classroom instruction (e.g. Moss, 2003; Brookhart, 2003; Mcmillan, 2003). Hence, every interaction between student and teacher becomes evidence. The evidence can come from the different sources such that formative assessment is both at individual and group level. The observation is critical and embedded in the act of teaching and assessment in classroom and success of observation contributes to the validity. For example, teacher can gather evidence about students thinking by asking questions and understanding the underlying reasoning of students’ responses (e.g. Coffey et al, 2011) or teacher can evaluate the individual student work on written assignments (e.g. authentic assessments, portfolios, quizzes) and these can be both at individual and group level. There is also a need to check for the elements of the formative assessment practice as it is specified in the definitions such as use of feedback and helping students to self-monitor their learning. 
Bennet (2011) in his system design connects these two steps to effectiveness of formative assessment. The first step is the validity argument and second is the efficacy argument. He refers to the formative hypothesis as forming an idea about the student based on current observation and revising it by collecting more data. In connection to Kane’s argument approach, the teacher accepts the conclusion about student if a set of possibilities (such as slips, mistakes) don’t count for explanation of student’s learning. Bennet states that validation requires the follow up assessment such as asking more questions and it requires strong cognitive domain model and content knowledge. He also points out the systematic and irrelevant sources to the inferences which may be connected to the students’ characteristics (e.g. gender, ELL status) where teachers actions to modify the instruction may be unintentionally biased. 
Toward a Coherent Theory  
I think that validity research can help how to shape the practice of formative assessment as a system by referring to both process and instrument sides with the aim of taking action to improve students’ learning by paralleling to instruction and curriculum. My conceptualization involves the use of means from validity theory by taking care of the aforementioned points in the context of formative assessment. I think it is possible to make a coherent argument about the system with a combination perspectives presented by Bennet (2011) which focuses on the inferences, and Brookhart (2003) and McMillan (2003) which focuses on the use of process elements. 
I would suggest adapting the concept of ‘process validity’ which is used in the literature of the drug quality (e.g. Food and Drug Administration, 2011). It emphasizes the ongoing nature of validation with recognition of more information about the process, necessitates the comprehensive process design, and incorporates the management into system (see Appendix A). 
The first stage requires the process design which involves understanding the learning environment, making arguments about the process and its elements (e.g. use of feedback), making hidden teacher assumptions explicit, and preparation of quality instruments. Shepard (2000) states that the learning environment is formed by teacher and student in such a way that they communicate about learning goals and what is important to learn. Namely, a learning environment which is not supportive would create a threat to the validity of the formative assessment. For example, collaborative learning situation needs the familiarity of students in such a work and appropriateness of tasks. Also, this stage also involves making hidden assumptions of teachers explicit (e.g. Kane, 2006) which includes teachers’ beliefs about how students learn that can shape the learning goals for students. Connectedly, the belief and implicit values of teachers can lead mistaken inferences about the students’ thinking. 
Additional arguments about both intended and unintended consequences of use of formative assessment elements can be made with respect to questions proposed by Shepard (1993). In this stage, the classroom practices are designed in connection to curriculum and relation between instruction and assessment is established. Development or specification of instruments is also made by content representativeness concept as defined by Brookhart (2003) and giving attention to threats defined by Messick (1983) where the quality of the instrument effects validation process because low quality delay the appropriate adjustment of instruction.  
 	The second stage is process qualification which refers to the performance in classroom where data is collected about the arguments from process design phase. Feed forward phase start in the learning environment and shaped by the interactions. Teachers need to make valid inferences about the students’ learning in an iterative way as described by Bennet (2011) and make adjustments for the instruction based on the feedback taken from students, and also promote the elements of formative assessment such as student self-monitoring. Teacher needs to collect data about unintended consequences (e.g. giving time to students to find supports for their current understandings may lead to strength their misconceptions). Also, questions such as appropriateness of challenging tasks or meaning of performance to student or use of feedback by student (McMillan, 2003) need to be searched by different tools in formative assessment.
Process verification refers the making evaluations about the second stage where instruction adjustments made with intend of promoting student learning. For example, teacher checks for whether feedback is helpful or leads confusion. This can be done concurrently with second stage by providing follow-up questions or in a later time such as next class or topic). Also, the information gathered from the second stage can be used for in the first phase of the next validation process for another topic in content domain.
This approach might provide teachers with a systematic perspective which is integrated in instruction and curriculum. Also, argument based structure in the system can be helpful to make teachers aware of the potential results and search for evidence which helps to become prepared for the adaptations in the case of any results.  
Significance 
This paper provides an overview on the concept of validity which is broadly discussed and examined in the context of high-stake assessments. The paper addresses the differences and challenges commonly mentioned in the formative assessment literature in connection of validity and it provides a validation process as a coherent framework. 






References
Bennett, R. E. (2011). Formative assessment: a critical review. Assessment in Education: Principles, Policy & Practice, 18(1), 5–25. 
Black, P., & Wiliam, D. (1998). Assessment and Classroom Learning. Assessment in Education, 5(1), 7–74..
Black, P., & Wiliam, D. (2009). Developing the theory of formative assessment. Educational Assessment, Evaluation and Accountability, 21, 5–31.
Borsboom, D., Mellenbergh, G.J., & Van Heerden, J. (2004). The concept of validity.
Psychological Review, 111, 1061–1071.
Brookhart, S.M. (2003). Developing Measurement Theory for Classroom Assessment Purposes and Uses. Educational Measurement: Issues and Practice, 22(4), 5 – 12. 
Brookhart, S.M. (2004). Classroom Assessment: Tensions and Intersections in Theory and Practice. Teachers College Record, 106(3), 429-458.
Coffey, J.E., Hammer, D., Levin, D.M. & Grant, T. (2011) The Missing Disciplinary Substance of Formative Assessment. Journal of Research in Science Teaching, 48(10), 1109-1136.
Food and Drug Administration (2011). Guidance for Industry Process Validation: General 
Principles and Practices. Retrieved March,2013 http://www.fda.gov/downloads/Drugs/GuidanceComplianceRegulatoryInformation/Guidances/ucm070336.pdf
Furtak, E. M., Cartun, A., Chrzanowski, A., Circi, R., Grover, R., Heredia,S. C.,……,White 
H.O.K. (2013). Manuscript in preparation.
Kane, M. T. (2006). Validation. In R. L. Brennan (Ed.), Educational measurement (4th ed., pp. 
17-64). Washington, DC: The National Council on Measurement in Education & the American Council on Education.
McMillan, J. H. (2003). Understanding and Improving Teachers' Classroom Assessment Decision Making: Implications for Theory and Practice. Educational Measurement: Issues and Practice, 22(4), 34 – 43.
Messick, S. (1989). Validity. In R. L. Linn (Ed.), Educational measurement (3rd ed., pp. 
13-103). Washington, DC: The American Council on Education & the National 
Council on Measurement in Education.
Moss, P. A. (2003). Reconceptualizing Validity for Classroom Assessment. Educational Measurement: Issues and Practice, 22(4), 13 – 25. 
Shepard, L. A. (1993). Evaluating test validity. In L. Darling-Hammond (Ed.), Review 
of Research in Education (Vol. 19, pp. 405-450). Washington, DC: American 
Educational Research Association
Shepard, L. A.  (2000). The Role of Assessment in a Learning Culture. Educational Researcher, 29(7), 4-14.
Shepard, L.A. (2003). Reconsidering Large-Scale Assessment to Heighten Its Relevance to 
Learning. In J. M. Atkin & J. E. Coffey (Eds.), Everyday Assessment in the Science Classroom. Arlington, VA: National Science Teachers Association.
